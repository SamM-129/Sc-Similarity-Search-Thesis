{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88952a09-a708-4db5-9ab2-28e0f0544916",
   "metadata": {},
   "source": [
    "This notebook collects query time and recall in the baseline solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53fa64d2-a07f-45da-a1be-aae39c9e0b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import faiss\n",
    "import numpy as np\n",
    "import itertools\n",
    "import statistics\n",
    "import matplotlib.cm as cm\n",
    "import time\n",
    "import os\n",
    "from time import perf_counter\n",
    "from time import perf_counter_ns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c10c1ac7-9daf-41ba-8fa4-958ab88aefc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['embed_all', 'embed_raw', 'embed_l2_norm', 'restore_order', 'embed_correct_coverage_fh', 'embed_l2_norm_correct_coverage_fh'])\n",
      "dict_keys(['batch id', 'age', 'total_cg', 'average_cg_rate', 'total_ch', 'average_ch_rate', 'hic_counts', 'cell_name_higashi', 'major', 'minor', 'cluster label', 'cluster label minor'])\n"
     ]
    }
   ],
   "source": [
    "# read in embeddings and cluster label info\n",
    "with open('embedding64.pickle', 'rb') as fp:\n",
    "    embedding64 = pickle.load(fp)\n",
    "with open('label_info.pickle', 'rb') as fp:\n",
    "    label_info = pickle.load(fp)\n",
    "\n",
    "print(embedding64.keys())\n",
    "print(label_info.keys())\n",
    "\n",
    "# create helpful dicts for the cluster labels\n",
    "# also look at n counts for the clusters\n",
    "major_labels = list(set(label_info['cluster label']))\n",
    "minor_labels = list(set(label_info['cluster label minor']))\n",
    "\n",
    "major = dict(zip(major_labels,[[] for item in major_labels]))\n",
    "minor = dict(zip(minor_labels,[[] for item in minor_labels]))\n",
    "\n",
    "true_major = label_info['cluster label']\n",
    "true_minor = label_info['cluster label minor']\n",
    "\n",
    "for j in range(len(label_info['cluster label'])):\n",
    "    maj = label_info['cluster label'][j]\n",
    "    min = label_info['cluster label minor'][j]\n",
    "\n",
    "    if maj in major.keys():\n",
    "        major[maj].append(j)\n",
    "    if min in minor.keys():\n",
    "        minor[min].append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "805db8d2-a008-440a-bef2-64e99b257c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get memory footprint for index\n",
    "# source: https://www.pinecone.io/learn/series/faiss/product-quantization/\n",
    "def get_memory(index):\n",
    "    faiss.write_index(index,'./temp.index')\n",
    "    file_size = os.path.getsize('./temp.index')\n",
    "    os.remove('./temp.index')\n",
    "    return file_size\n",
    "    \n",
    "# function to calculate recall based on search results\n",
    "def get_recall_min(i, result_min):\n",
    "    # recall: TP / cluster size\n",
    "    return len(set(minor[true_minor[i]]).intersection(result_min)) / len(minor[true_minor[i]])\n",
    "\n",
    "def get_recall_maj(i, result_maj):\n",
    "    # recall: TP / cluster size\n",
    "    return len(set(major[true_major[i]]).intersection(result_maj)) / len(major[true_major[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59417a93-1abc-4424-9aeb-f7415196719c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create input database\n",
    "database = np.array(embedding64[\"embed_l2_norm\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc490aa8-689f-4c53-a1d4-e3b88d235e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_rep(index, query, k):\n",
    "    start1 = perf_counter_ns()\n",
    "    for x in range(100):\n",
    "        D, I = index.search(query, k)\n",
    "    end1 = perf_counter_ns()\n",
    "        \n",
    "    start2 = perf_counter_ns()\n",
    "    for x in range(100):\n",
    "        D, I = index.search(query, k)\n",
    "    end2 = perf_counter_ns()\n",
    "        \n",
    "    start3 = perf_counter_ns()\n",
    "    for x in range(100):\n",
    "        D, I = index.search(query, k)\n",
    "    end3 = perf_counter_ns()\n",
    "\n",
    "    times = [(end1-start1),(end2-start2),(end3-start3)]\n",
    "    times.sort()\n",
    "\n",
    "    return I, times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ede2e358-eaeb-429b-b164-31fa2a141ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define experiment function\n",
    "def baseline_experiment():\n",
    "    # initialize empty arrays for results\n",
    "    recall_min = np.zeros([4238])\n",
    "    recall_maj = np.zeros([4238])\n",
    "    speed_min = np.zeros([4238])\n",
    "    speed_maj = np.zeros([4238])\n",
    "\n",
    "    # create flat index\n",
    "    index = faiss.IndexFlatL2(64)\n",
    "    index.add(database)\n",
    "    \n",
    "    # how much memory is used?\n",
    "    memory = np.array(get_memory(index))\n",
    "\n",
    "    for i in range(4238):\n",
    "        query = np.array([database[i]])\n",
    "        k_major = len(major[true_major[i]]) # size of true cluster (how many neighbors to return)\n",
    "        k_minor = len(minor[true_minor[i]]) # size of true cluster (how many neighbors to return)\n",
    "\n",
    "        # major cluster query\n",
    "        I, time = query_rep(index, query, k_major)\n",
    "        speed_maj[i] = time[0]\n",
    "        recall_maj[i] = get_recall_maj(i, I[0])\n",
    "\n",
    "        # minor cluster query\n",
    "        I, time = query_rep(index, query, k_minor)\n",
    "        recall_min[i] = get_recall_min(i, I[0])\n",
    "        speed_min[i] = time[0]\n",
    "\n",
    "    # return all results\n",
    "    return recall_min, recall_maj, speed_min, speed_maj, memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5ea9933-2822-455f-b633-e577a3d05c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_min_BASELINE, recall_maj_BASELINE, speed_min_BASELINE, speed_maj_BASELINE, memory_BASELINE = baseline_experiment()\n",
    "\n",
    "with open('FLAT/Baseline.npy', 'wb') as f:\n",
    "    np.save(f, recall_min_BASELINE)\n",
    "    np.save(f, recall_maj_BASELINE)\n",
    "    np.save(f, speed_min_BASELINE)    \n",
    "    np.save(f, speed_maj_BASELINE)\n",
    "    np.save(f, memory_BASELINE)\n",
    "\n",
    "# with open('FLAT/Baseline.npy', 'rb') as f:\n",
    "#     recall_min_BASELINE = np.load(f)\n",
    "#     recall_maj_BASELINE = np.load(f)\n",
    "#     speed_min_BASELINE = np.load(f)\n",
    "#     speed_maj_BASELINE = np.load(f)\n",
    "#     memory_BASELINE = np.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "178a04b0-4289-49fd-9ffa-d3d0e3681ac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7984170792592759\n",
      "0.6215873936903249\n",
      "11708813.992449269\n",
      "8936997.569608307\n"
     ]
    }
   ],
   "source": [
    "print(statistics.mean(recall_maj_BASELINE))\n",
    "print(statistics.mean(recall_min_BASELINE))\n",
    "print(statistics.mean(speed_maj_BASELINE))\n",
    "print(statistics.mean(speed_min_BASELINE))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
