{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c686f27f-646c-4510-a39f-6035aee5e3f8",
   "metadata": {},
   "source": [
    "This notebook collects query time, recall, and memory use in the hierarchical navigable small world index solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53fa64d2-a07f-45da-a1be-aae39c9e0b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import faiss\n",
    "import numpy as np\n",
    "import itertools\n",
    "import statistics\n",
    "import matplotlib.cm as cm\n",
    "import time\n",
    "import os\n",
    "from time import perf_counter_ns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c10c1ac7-9daf-41ba-8fa4-958ab88aefc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['embed_all', 'embed_raw', 'embed_l2_norm', 'restore_order', 'embed_correct_coverage_fh', 'embed_l2_norm_correct_coverage_fh'])\n",
      "dict_keys(['batch id', 'age', 'total_cg', 'average_cg_rate', 'total_ch', 'average_ch_rate', 'hic_counts', 'cell_name_higashi', 'major', 'minor', 'cluster label', 'cluster label minor'])\n"
     ]
    }
   ],
   "source": [
    "# read in embeddings and cluster label info\n",
    "with open('embedding64.pickle', 'rb') as fp:\n",
    "    embedding64 = pickle.load(fp)\n",
    "with open('label_info.pickle', 'rb') as fp:\n",
    "    label_info = pickle.load(fp)\n",
    "\n",
    "print(embedding64.keys())\n",
    "print(label_info.keys())\n",
    "\n",
    "# create helpful dicts for the cluster labels\n",
    "# also look at n counts for the clusters\n",
    "major_labels = list(set(label_info['cluster label']))\n",
    "minor_labels = list(set(label_info['cluster label minor']))\n",
    "\n",
    "major = dict(zip(major_labels,[[] for item in major_labels]))\n",
    "minor = dict(zip(minor_labels,[[] for item in minor_labels]))\n",
    "\n",
    "true_major = label_info['cluster label']\n",
    "true_minor = label_info['cluster label minor']\n",
    "\n",
    "for j in range(len(label_info['cluster label'])):\n",
    "    maj = label_info['cluster label'][j]\n",
    "    min = label_info['cluster label minor'][j]\n",
    "\n",
    "    if maj in major.keys():\n",
    "        major[maj].append(j)\n",
    "    if min in minor.keys():\n",
    "        minor[min].append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "805db8d2-a008-440a-bef2-64e99b257c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get memory footprint for index\n",
    "# source: https://www.pinecone.io/learn/series/faiss/product-quantization/\n",
    "def get_memory(index):\n",
    "    faiss.write_index(index,'./temp.index')\n",
    "    file_size = os.path.getsize('./temp.index')\n",
    "    os.remove('./temp.index')\n",
    "    return file_size\n",
    "    \n",
    "# function to calculate recall based on search results\n",
    "def get_recall_min(i, result_min):\n",
    "    # recall: TP / cluster size\n",
    "    return len(set(minor[true_minor[i]]).intersection(result_min)) / len(minor[true_minor[i]])\n",
    "\n",
    "def get_recall_maj(i, result_maj):\n",
    "    # recall: TP / cluster size\n",
    "    return len(set(major[true_major[i]]).intersection(result_maj)) / len(major[true_major[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59417a93-1abc-4424-9aeb-f7415196719c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create input database\n",
    "database = np.array(embedding64[\"embed_l2_norm\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "96a54b7f-b456-4d33-9133-79c6c5b3fdd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_rep(index, query, k):\n",
    "    start1 = perf_counter_ns()\n",
    "    for x in range(100):\n",
    "        D, I = index.search(query, k)\n",
    "    end1 = perf_counter_ns()\n",
    "        \n",
    "    start2 = perf_counter_ns()\n",
    "    for x in range(100):\n",
    "        D, I = index.search(query, k)\n",
    "    end2 = perf_counter_ns()\n",
    "        \n",
    "    start3 = perf_counter_ns()\n",
    "    for x in range(100):\n",
    "        D, I = index.search(query, k)\n",
    "    end3 = perf_counter_ns()\n",
    "\n",
    "    times = [(end1-start1),(end2-start2),(end3-start3)]\n",
    "    times.sort()\n",
    "\n",
    "    return I, times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ede2e358-eaeb-429b-b164-31fa2a141ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define experiment function\n",
    "def HNSW_experiment(index_str):\n",
    "    # initialize empty arrays for results\n",
    "    recall_min = np.zeros([4238])\n",
    "    recall_maj = np.zeros([4238])\n",
    "    speed_min = np.zeros([4238])\n",
    "    speed_maj = np.zeros([4238])\n",
    "    memory = np.zeros([4238])\n",
    "    \n",
    "    for i in range(4238):\n",
    "        # subset dataset\n",
    "        db_subset = np.delete(database, i, 0)\n",
    "        query = np.array([database[i]])\n",
    "        k_major = len(major[true_major[i]]) # size of true cluster (how many neighbors to return)\n",
    "        k_minor = len(minor[true_minor[i]]) # size of true cluster (how many neighbors to return)\n",
    "        \n",
    "        # create and train index\n",
    "        index = faiss.index_factory(64, index_str)\n",
    "        index.train(db_subset)\n",
    "        index.add(database)\n",
    "\n",
    "        # how much memory is used?\n",
    "        memory[i] = get_memory(index)\n",
    "       \n",
    "        # major cluster query\n",
    "        I, time = query_rep(index, query, k_major)\n",
    "        recall_maj[i] = get_recall_maj(i, I[0])\n",
    "        speed_maj[i] = time[0]\n",
    "\n",
    "        # minor cluster query\n",
    "        I, time = query_rep(index, query, k_minor)\n",
    "        recall_min[i] = get_recall_min(i, I[0])\n",
    "        speed_min[i] = time[0]        \n",
    "\n",
    "    # return all results\n",
    "    return recall_min, recall_maj, speed_min, speed_maj, memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e5ea9933-2822-455f-b633-e577a3d05c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "### EXPERIMENT A: M=16\n",
    "recall_min_A, recall_maj_A, speed_min_A, speed_maj_A, memory_A = HNSW_experiment(\"HNSW16,Flat\")\n",
    "\n",
    "with open('HNSW/ExperimentA.npy', 'wb') as f:\n",
    "    np.save(f, recall_min_A)\n",
    "    np.save(f, recall_maj_A)\n",
    "    np.save(f, speed_min_A)\n",
    "    np.save(f, speed_maj_A)\n",
    "    np.save(f, memory_A)\n",
    "\n",
    "# with open('HNSW/ExperimentA.npy', 'rb') as f:\n",
    "#     recall_min_A = np.load(f)\n",
    "#     recall_maj_A = np.load(f)\n",
    "#     speed_min_A = np.load(f)\n",
    "#     speed_maj_A = np.load(f)\n",
    "#     memory_A = np.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a1b8cb7e-1b63-4070-a55f-a4723580daf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### EXPERIMENT B: M=32\n",
    "recall_min_B, recall_maj_B, speed_min_B, speed_maj_B, memory_B = HNSW_experiment(\"HNSW32,Flat\")\n",
    "\n",
    "with open('HNSW/ExperimentB.npy', 'wb') as f:\n",
    "    np.save(f, recall_min_B)\n",
    "    np.save(f, recall_maj_B)\n",
    "    np.save(f, speed_min_B)\n",
    "    np.save(f, speed_maj_B)\n",
    "    np.save(f, memory_B)\n",
    "\n",
    "# with open('HNSW/ExperimentB.npy', 'rb') as f:\n",
    "#     recall_min_B = np.load(f)\n",
    "#     recall_maj_B = np.load(f)\n",
    "#     speed_min_B = np.load(f)\n",
    "#     speed_maj_B = np.load(f)\n",
    "#     memory_B = np.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "23d5ea29-d9b4-499d-9c5f-35bb1db73ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "### EXPERIMENT C: M=64\n",
    "recall_min_C, recall_maj_C, speed_min_C, speed_maj_C, memory_C = HNSW_experiment(\"HNSW64,Flat\")\n",
    "\n",
    "with open('HNSW/ExperimentC.npy', 'wb') as f:\n",
    "    np.save(f, recall_min_C)\n",
    "    np.save(f, recall_maj_C)\n",
    "    np.save(f, speed_min_C)\n",
    "    np.save(f, speed_maj_C)\n",
    "    np.save(f, memory_C)\n",
    "\n",
    "# with open('HNSW/ExperimentC.npy', 'rb') as f:\n",
    "#     recall_min_C = np.load(f)\n",
    "#     recall_maj_C = np.load(f)\n",
    "#     speed_min_C = np.load(f)\n",
    "#     speed_maj_C = np.load(f)\n",
    "#     memory_C = np.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8e66a7af-d532-40a2-98db-e4a1c8c8a25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### EXPERIMENT D: M=128\n",
    "recall_min_D, recall_maj_D, speed_min_D, speed_maj_D, memory_D = HNSW_experiment(\"HNSW128,Flat\")\n",
    "\n",
    "with open('HNSW/ExperimentD.npy', 'wb') as f:\n",
    "    np.save(f, recall_min_D)\n",
    "    np.save(f, recall_maj_D)\n",
    "    np.save(f, speed_min_D)\n",
    "    np.save(f, speed_maj_D)\n",
    "    np.save(f, memory_D)\n",
    "\n",
    "# with open('HNSW/ExperimentD.npy', 'rb') as f:\n",
    "#     recall_min_D = np.load(f)\n",
    "#     recall_maj_D = np.load(f)\n",
    "#     speed_min_D = np.load(f)\n",
    "#     speed_maj_D = np.load(f)\n",
    "#     memory_D = np.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2900679e-bb54-4031-b776-56f290d9f437",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
